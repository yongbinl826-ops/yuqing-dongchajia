"""
Chinese sentiment analysis using BERT and keyword extraction using Jieba + TF-IDF
"""

import logging
from typing import Dict, List, Tuple, Optional
import numpy as np
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
import re

logger = logging.getLogger(__name__)

class ChineseSentimentAnalyzer:
    def __init__(self, model_name: str = "bert-base-chinese"):
        """
        Initialize Chinese sentiment analyzer with BERT model
        
        Args:
            model_name: Hugging Face model name for sentiment analysis
        """
        self.model_name = model_name
        self.device = 0 if torch.cuda.is_available() else -1
        
        try:
            # Load sentiment analysis pipeline
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model=model_name,
                device=self.device,
                truncation=True,
                max_length=512
            )
            logger.info(f"Sentiment analysis model loaded: {model_name}")
        except Exception as e:
            logger.error(f"Error loading sentiment model: {str(e)}")
            raise

    def analyze_sentiment(self, text: str) -> Dict:
        """
        Analyze sentiment of Chinese text
        
        Args:
            text: Chinese text to analyze
            
        Returns:
            Dictionary with sentiment label and score
        """
        try:
            # Clean text
            text = self._clean_text(text)
            
            if not text or len(text.strip()) == 0:
                return {
                    "sentiment": "neutral",
                    "score": 0.5,
                    "confidence": 0.0,
                    "label": "neutral"
                }
            
            # Get sentiment prediction
            result = self.sentiment_pipeline(text[:512])[0]
            
            # Map BERT output labels to our sentiment categories
            label = result["label"]
            score = result["score"]
            
            # BERT typically outputs: NEGATIVE (label 0), NEUTRAL (label 1), POSITIVE (label 2)
            # Or sometimes: negative, neutral, positive
            sentiment_map = {
                "NEGATIVE": "negative",
                "NEUTRAL": "neutral",
                "POSITIVE": "positive",
                "negative": "negative",
                "neutral": "neutral",
                "positive": "positive",
                "0": "negative",
                "1": "neutral",
                "2": "positive",
            }
            
            sentiment = sentiment_map.get(label, "neutral")
            
            # Normalize score to 0-1 range
            normalized_score = float(score)
            
            return {\n                "sentiment": sentiment,\n                "score": normalized_score,\n                "confidence": normalized_score,\n                "label": label,\n                "raw_result": result\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing sentiment: {str(e)}\")\n            return {\n                \"sentiment\": \"neutral\",\n                \"score\": 0.5,\n                \"confidence\": 0.0,\n                \"error\": str(e)\n            }\n\n    def batch_analyze_sentiment(self, texts: List[str]) -> List[Dict]:\n        \"\"\"\n        Analyze sentiment for multiple texts\n        \n        Args:\n            texts: List of Chinese texts\n            \n        Returns:\n            List of sentiment analysis results\n        \"\"\"\n        results = []\n        for text in texts:\n            result = self.analyze_sentiment(text)\n            results.append(result)\n        return results\n\n    def _clean_text(self, text: str) -> str:\n        \"\"\"\n        Clean and preprocess text\n        \n        Args:\n            text: Raw text\n            \n        Returns:\n            Cleaned text\n        \"\"\"\n        # Remove URLs\n        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n        # Remove email addresses\n        text = re.sub(r\"\\S+@\\S+\", \"\", text)\n        # Remove special characters but keep Chinese characters, English, numbers\n        text = re.sub(r\"[^\\u4e00-\\u9fff\\w\\s]\", \"\", text)\n        # Remove extra whitespace\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n\n\nclass ChineseKeywordExtractor:\n    def __init__(self, max_keywords: int = 10):\n        \"\"\"\n        Initialize keyword extractor\n        \n        Args:\n            max_keywords: Maximum number of keywords to extract\n        \"\"\"\n        self.max_keywords = max_keywords\n        self.tfidf_vectorizer = None\n        logger.info(\"ChineseKeywordExtractor initialized\")\n\n    def extract_keywords(self, text: str) -> List[Tuple[str, float]]:\n        \"\"\"\n        Extract keywords from Chinese text using Jieba + TF-IDF\n        \n        Args:\n            text: Chinese text\n            \n        Returns:\n            List of (keyword, score) tuples\n        \"\"\"\n        try:\n            # Segment text with Jieba\n            words = jieba.cut(text)\n            \n            # Filter stopwords and short words\n            filtered_words = [\n                w for w in words\n                if len(w) > 1 and not self._is_stopword(w)\n            ]\n            \n            if not filtered_words:\n                return []\n            \n            # Calculate TF-IDF scores\n            vectorizer = TfidfVectorizer(\n                analyzer=\"char\",\n                ngram_range=(1, 2),\n                max_features=self.max_keywords * 2\n            )\n            \n            try:\n                tfidf_matrix = vectorizer.fit_transform([\" \".join(filtered_words)])\n                feature_names = vectorizer.get_feature_names_out()\n                scores = tfidf_matrix.toarray()[0]\n                \n                # Get top keywords\n                top_indices = scores.argsort()[-self.max_keywords:][::-1]\n                keywords = [\n                    (feature_names[i], float(scores[i]))\n                    for i in top_indices\n                    if scores[i] > 0\n                ]\n                \n                return keywords\n            except Exception as e:\n                logger.warning(f\"TF-IDF calculation failed: {str(e)}\")\n                # Fallback: return word frequencies\n                from collections import Counter\n                word_freq = Counter(filtered_words)\n                return [\n                    (word, freq / len(filtered_words))\n                    for word, freq in word_freq.most_common(self.max_keywords)\n                ]\n            \n        except Exception as e:\n            logger.error(f\"Error extracting keywords: {str(e)}\")\n            return []\n\n    def extract_keywords_batch(self, texts: List[str]) -> List[List[Tuple[str, float]]]:\n        \"\"\"\n        Extract keywords from multiple texts\n        \n        Args:\n            texts: List of Chinese texts\n            \n        Returns:\n            List of keyword lists\n        \"\"\"\n        results = []\n        for text in texts:\n            keywords = self.extract_keywords(text)\n            results.append(keywords)\n        return results\n\n    def _is_stopword(self, word: str) -> bool:\n        \"\"\"\n        Check if word is a stopword\n        \n        Args:\n            word: Word to check\n            \n        Returns:\n            True if word is a stopword\n        \"\"\"\n        # Common Chinese stopwords\n        stopwords = {\n            \"的\", \"一\", \"是\", \"在\", \"不\", \"了\", \"有\", \"和\", \"人\", \"这\",\n            \"中\", \"大\", \"为\", \"上\", \"个\", \"国\", \"我\", \"以\", \"要\", \"他\",\n            \"时\", \"来\", \"用\", \"们\", \"生\", \"到\", \"作\", \"地\", \"于\", \"出\",\n            \"就\", \"分\", \"对\", \"成\", \"会\", \"可\", \"主\", \"发\", \"年\", \"动\",\n            \"同\", \"工\", \"也\", \"能\", \"下\", \"过\", \"民\", \"前\", \"面\", \"书\",\n            \"水\", \"后\", \"去\", \"种\", \"把\", \"多\", \"然\", \"前\", \"只\", \"其\",\n            \"现\", \"起\", \"敢\", \"比\", \"最\", \"每\", \"那\", \"沿\", \"见\", \"坚\",\n            \"反\", \"却\", \"都\", \"放\", \"第\", \"她\", \"从\", \"给\", \"向\", \"得\",\n            \"很\", \"或\", \"叫\", \"讲\", \"第\", \"样\", \"想\", \"作\", \"对\", \"开\",\n            \"经\", \"分\", \"别\", \"她\", \"随\", \"口\", \"新\", \"每\", \"法\", \"平\"\n        }\n        return word in stopwords\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Initialize analyzer\n    sentiment_analyzer = ChineseSentimentAnalyzer()\n    keyword_extractor = ChineseKeywordExtractor()\n    \n    # Test sentiment analysis\n    test_texts = [\n        \"这个产品真的很好用，我非常喜欢！\",\n        \"太糟糕了，完全不能用。\",\n        \"还可以，一般般。\"\n    ]\n    \n    print(\"Sentiment Analysis Results:\")\n    for text in test_texts:\n        result = sentiment_analyzer.analyze_sentiment(text)\n        print(f\"Text: {text}\")\n        print(f\"Sentiment: {result['sentiment']}, Score: {result['score']:.4f}\")\n        print()\n    \n    # Test keyword extraction\n    print(\"\\nKeyword Extraction Results:\")\n    for text in test_texts:\n        keywords = keyword_extractor.extract_keywords(text)\n        print(f\"Text: {text}\")\n        print(f\"Keywords: {keywords}\")\n        print()\n
